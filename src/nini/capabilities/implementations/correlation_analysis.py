"""ç›¸å…³æ€§åˆ†æèƒ½åŠ›å®ç°ã€‚

æ‰§è¡Œå®Œæ•´çš„ç›¸å…³æ€§åˆ†ææµç¨‹ï¼š
1. æ•°æ®éªŒè¯ä¸åˆ—ç±»å‹æ£€æŸ¥
2. è‡ªåŠ¨é€‰æ‹©ç›¸å…³æ–¹æ³•ï¼ˆPearson / Spearman / Kendallï¼‰
3. æ­£æ€æ€§æ£€éªŒï¼ˆç”¨äºæ–¹æ³•é€‰æ‹©ï¼‰
4. è®¡ç®—ç›¸å…³çŸ©é˜µä¸ p å€¼çŸ©é˜µ
5. å¤šé‡æ¯”è¾ƒæ ¡æ­£
6. å¯è§†åŒ–ï¼ˆçƒ­åŠ›å›¾ï¼‰
7. ç”Ÿæˆè§£é‡Šæ€§æŠ¥å‘Š
"""

from __future__ import annotations

import math
from dataclasses import dataclass, field
from typing import TYPE_CHECKING, Any

import pandas as pd
from scipy import stats

if TYPE_CHECKING:
    from nini.agent.session import Session
    from nini.tools.base import SkillResult


@dataclass
class CorrelationPair:
    """å•ä¸ªå˜é‡å¯¹çš„ç›¸å…³æ€§ç»“æœã€‚"""

    var1: str
    var2: str
    coefficient: float
    p_value: float
    p_adjusted: float | None = None
    significant: bool = False
    strength: str = ""  # weak / moderate / strong


@dataclass
class CorrelationAnalysisResult:
    """ç›¸å…³æ€§åˆ†æç»“æœã€‚"""

    success: bool = False
    message: str = ""

    # åˆ†æå‚æ•°
    method: str = ""
    method_reason: str = ""
    n_variables: int = 0
    sample_size: int = 0
    columns: list[str] = field(default_factory=list)

    # æ­£æ€æ€§æ£€éªŒ
    normality_tests: dict[str, Any] = field(default_factory=dict)

    # ç›¸å…³çŸ©é˜µ
    correlation_matrix: dict[str, dict[str, float]] = field(default_factory=dict)
    pvalue_matrix: dict[str, dict[str, float]] = field(default_factory=dict)

    # æ˜¾è‘—é…å¯¹ï¼ˆç»å¤šé‡æ¯”è¾ƒæ ¡æ­£åï¼‰
    significant_pairs: list[CorrelationPair] = field(default_factory=list)
    all_pairs: list[CorrelationPair] = field(default_factory=list)

    # å¤šé‡æ¯”è¾ƒæ ¡æ­£
    correction_method: str = ""
    alpha: float = 0.05

    # å¯è§†åŒ–
    chart_artifact: dict[str, Any] | None = None

    # è§£é‡Šæ€§æŠ¥å‘Š
    interpretation: str = ""

    def to_dict(self) -> dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸ã€‚"""
        return {
            "success": self.success,
            "message": self.message,
            "method": self.method,
            "method_reason": self.method_reason,
            "n_variables": self.n_variables,
            "sample_size": self.sample_size,
            "columns": self.columns,
            "normality_tests": self.normality_tests,
            "correlation_matrix": self.correlation_matrix,
            "pvalue_matrix": self.pvalue_matrix,
            "significant_pairs": [
                {
                    "var1": p.var1,
                    "var2": p.var2,
                    "coefficient": p.coefficient,
                    "p_value": p.p_value,
                    "p_adjusted": p.p_adjusted,
                    "significant": p.significant,
                    "strength": p.strength,
                }
                for p in self.significant_pairs
            ],
            "all_pairs": [
                {
                    "var1": p.var1,
                    "var2": p.var2,
                    "coefficient": p.coefficient,
                    "p_value": p.p_value,
                    "p_adjusted": p.p_adjusted,
                    "significant": p.significant,
                    "strength": p.strength,
                }
                for p in self.all_pairs
            ],
            "correction_method": self.correction_method,
            "alpha": self.alpha,
            "chart_artifact": self.chart_artifact,
            "interpretation": self.interpretation,
        }


class CorrelationAnalysisCapability:
    """
    ç›¸å…³æ€§åˆ†æèƒ½åŠ›ã€‚

    è‡ªåŠ¨æ‰§è¡Œå®Œæ•´çš„ç›¸å…³æ€§åˆ†ææµç¨‹ï¼ŒåŒ…æ‹¬ï¼š
    - æ•°æ®éªŒè¯ä¸åˆ—ç±»å‹æ£€æŸ¥
    - æ­£æ€æ€§æ£€éªŒï¼ˆå†³å®šä½¿ç”¨ Pearson æˆ– Spearmanï¼‰
    - è®¡ç®—ç›¸å…³çŸ©é˜µä¸ p å€¼çŸ©é˜µ
    - Bonferroni å¤šé‡æ¯”è¾ƒæ ¡æ­£
    - çƒ­åŠ›å›¾å¯è§†åŒ–
    - è§£é‡Šæ€§æŠ¥å‘Š

    ä½¿ç”¨æ–¹æ³•ï¼š
        capability = CorrelationAnalysisCapability()
        result = await capability.execute(
            session,
            dataset_name="my_data",
            columns=["var1", "var2", "var3"]
        )
    """

    def __init__(self, registry: Any | None = None) -> None:
        self.name = "correlation_analysis"
        self.display_name = "ç›¸å…³æ€§åˆ†æ"
        self.description = "æ¢ç´¢å˜é‡ä¹‹é—´çš„ç›¸å…³å…³ç³»"
        self.icon = "ğŸ“ˆ"
        self._registry = registry

    async def execute(
        self,
        session: Session,
        *,
        dataset_name: str,
        columns: list[str] | None = None,
        method: str = "auto",
        alpha: float = 0.05,
        correction: str = "bonferroni",
        **kwargs: Any,
    ) -> CorrelationAnalysisResult:
        """
        æ‰§è¡Œç›¸å…³æ€§åˆ†æã€‚

        Args:
            session: ä¼šè¯å¯¹è±¡
            dataset_name: æ•°æ®é›†åç§°
            columns: è¦åˆ†æçš„åˆ—ååˆ—è¡¨ï¼ˆNone åˆ™è‡ªåŠ¨é€‰æ‹©æ‰€æœ‰æ•°å€¼åˆ—ï¼‰
            method: ç›¸å…³æ–¹æ³• ("auto", "pearson", "spearman", "kendall")
            alpha: æ˜¾è‘—æ€§æ°´å¹³
            correction: å¤šé‡æ¯”è¾ƒæ ¡æ­£æ–¹æ³• ("bonferroni", "none")
        """
        result = CorrelationAnalysisResult(alpha=alpha, correction_method=correction)

        # Step 1: æ•°æ®éªŒè¯
        df = session.datasets.get(dataset_name)
        if df is None:
            result.message = f"æ•°æ®é›† '{dataset_name}' ä¸å­˜åœ¨"
            return result

        # Step 2: ç¡®å®šåˆ†æåˆ—
        if columns is None:
            columns = [col for col in df.columns if pd.api.types.is_numeric_dtype(df[col])]
            if len(columns) < 2:
                result.message = "æ•°æ®é›†ä¸­æ•°å€¼åˆ—ä¸è¶³ 2 åˆ—ï¼Œæ— æ³•è¿›è¡Œç›¸å…³æ€§åˆ†æ"
                return result

        if not self._validate_columns(df, columns, result):
            return result

        result.columns = columns
        result.n_variables = len(columns)

        # Step 3: å‡†å¤‡å¹²å‡€æ•°æ®
        clean_data = df[columns].dropna()
        result.sample_size = len(clean_data)
        if result.sample_size < 3:
            result.message = "å®Œæ•´è§‚æµ‹å€¼ä¸è¶³ 3 ä¸ªï¼Œæ— æ³•è¿›è¡Œç›¸å…³æ€§åˆ†æ"
            return result

        # Step 4: æ­£æ€æ€§æ£€éªŒ â†’ é€‰æ‹©æ–¹æ³•
        if method == "auto":
            normality = self._check_normality(clean_data, columns, alpha)
            result.normality_tests = normality
            selected_method = self._select_method(normality)
            result.method = selected_method
            result.method_reason = self._get_method_reason(selected_method, normality)
        else:
            result.method = method
            result.method_reason = f"ç”¨æˆ·æŒ‡å®šä½¿ç”¨ {method.title()} æ–¹æ³•"

        # Step 5: è°ƒç”¨åº•å±‚å·¥å…·è®¡ç®—ç›¸å…³çŸ©é˜µ
        corr_result = await self._compute_correlation(session, dataset_name, columns, result.method)
        if corr_result is None:
            result.message = "ç›¸å…³æ€§è®¡ç®—å¤±è´¥"
            return result

        result.correlation_matrix = corr_result.get("correlation_matrix", {})
        result.pvalue_matrix = corr_result.get("pvalue_matrix", {})

        # Step 6: æå–é…å¯¹å¹¶åšå¤šé‡æ¯”è¾ƒæ ¡æ­£
        result.all_pairs = self._extract_pairs(
            columns, result.correlation_matrix, result.pvalue_matrix
        )
        n_comparisons = len(result.all_pairs)
        self._apply_correction(result.all_pairs, correction, n_comparisons, alpha)
        result.significant_pairs = [p for p in result.all_pairs if p.significant]

        # Step 7: å¯è§†åŒ–ï¼ˆä½¿ç”¨å·²è®¡ç®—çš„ç›¸å…³çŸ©é˜µï¼Œé¿å…ä¸æ–‡æœ¬ç»“æœä¸ä¸€è‡´ï¼‰
        chart_result = await self._create_heatmap(
            session, dataset_name, columns, result.method, result.correlation_matrix
        )
        if chart_result is not None:
            if isinstance(chart_result, dict):
                if chart_result.get("success") and chart_result.get("artifacts"):
                    result.chart_artifact = chart_result["artifacts"][0]
            elif hasattr(chart_result, "success") and chart_result.success:
                if chart_result.artifacts:
                    result.chart_artifact = chart_result.artifacts[0]

        # Step 8: è®°å½•å¯Œä¿¡æ¯åˆ° AnalysisMemory
        self._record_enriched_result(session, dataset_name, result)

        # Step 9: ç”Ÿæˆè§£é‡Š
        result.interpretation = self._generate_interpretation(result)
        result.success = True
        result.message = "ç›¸å…³æ€§åˆ†æå®Œæˆ"
        return result

    def _validate_columns(
        self,
        df: pd.DataFrame,
        columns: list[str],
        result: CorrelationAnalysisResult,
    ) -> bool:
        """éªŒè¯åˆ—æœ‰æ•ˆæ€§ã€‚"""
        if len(columns) < 2:
            result.message = "è‡³å°‘éœ€è¦ 2 ä¸ªå˜é‡è¿›è¡Œç›¸å…³æ€§åˆ†æ"
            return False

        for col in columns:
            if col not in df.columns:
                result.message = f"åˆ— '{col}' ä¸å­˜åœ¨"
                return False
            if not pd.api.types.is_numeric_dtype(df[col]):
                result.message = f"åˆ— '{col}' ä¸æ˜¯æ•°å€¼ç±»å‹"
                return False
        return True

    def _check_normality(
        self,
        data: pd.DataFrame,
        columns: list[str],
        alpha: float,
    ) -> dict[str, Any]:
        """å¯¹æ¯åˆ—è¿›è¡Œ Shapiro-Wilk æ­£æ€æ€§æ£€éªŒã€‚"""
        normality: dict[str, Any] = {}
        for col in columns:
            col_data = data[col].dropna()
            if len(col_data) < 3 or len(col_data) > 5000:
                normality[col] = {
                    "tested": False,
                    "reason": "æ ·æœ¬é‡ä¸åœ¨ Shapiro-Wilk é€‚ç”¨èŒƒå›´å†…",
                }
                continue
            try:
                statistic, p_value = stats.shapiro(col_data)
                normality[col] = {
                    "tested": True,
                    "statistic": float(statistic),
                    "p_value": float(p_value),
                    "normal": bool(p_value > alpha),
                }
            except Exception as exc:
                normality[col] = {
                    "tested": False,
                    "reason": f"æ­£æ€æ€§æ£€éªŒå¤±è´¥: {exc}",
                }
        return normality

    def _select_method(self, normality: dict[str, Any]) -> str:
        """æ ¹æ®æ­£æ€æ€§æ£€éªŒç»“æœé€‰æ‹©æ–¹æ³•ã€‚"""
        all_normal = True
        for col, result in normality.items():
            if not isinstance(result, dict):
                continue
            if result.get("tested") is False:
                continue
            if not result.get("normal", True):
                all_normal = False
                break
        return "pearson" if all_normal else "spearman"

    def _get_method_reason(self, method: str, normality: dict[str, Any]) -> str:
        """è·å–æ–¹æ³•é€‰æ‹©åŸå› ã€‚"""
        non_normal_cols = []
        for col, result in normality.items():
            if isinstance(result, dict) and result.get("tested") and not result.get("normal", True):
                non_normal_cols.append(col)

        if method == "pearson":
            return "æ‰€æœ‰å˜é‡é€šè¿‡æ­£æ€æ€§æ£€éªŒï¼Œä½¿ç”¨ Pearson ç›¸å…³ç³»æ•°"
        elif method == "spearman":
            cols_text = "ã€".join(non_normal_cols[:3])
            suffix = " ç­‰" if len(non_normal_cols) > 3 else ""
            return f"å˜é‡ {cols_text}{suffix} ä¸ç¬¦åˆæ­£æ€åˆ†å¸ƒï¼Œä½¿ç”¨ Spearman ç§©ç›¸å…³"
        return f"ä½¿ç”¨ {method.title()} æ–¹æ³•"

    def _get_registry(self) -> Any:
        """è·å–å·¥å…·æ³¨å†Œä¸­å¿ƒã€‚"""
        if self._registry is not None:
            return self._registry
        from nini.tools.registry import create_default_tool_registry

        return create_default_tool_registry()

    async def _compute_correlation(
        self,
        session: Session,
        dataset_name: str,
        columns: list[str],
        method: str,
    ) -> dict[str, Any] | None:
        """é€šè¿‡åº•å±‚å·¥å…·è®¡ç®—ç›¸å…³çŸ©é˜µã€‚"""
        registry = self._get_registry()
        try:
            tool_result = await registry.execute(
                "correlation",
                session,
                dataset_name=dataset_name,
                columns=columns,
                method=method,
            )
            if isinstance(tool_result, dict):
                if tool_result.get("success") and tool_result.get("data"):
                    return tool_result["data"]
            elif hasattr(tool_result, "success") and tool_result.success and tool_result.data:
                return tool_result.data
        except Exception:
            pass
        return None

    def _extract_pairs(
        self,
        columns: list[str],
        corr_matrix: dict[str, dict[str, float]],
        pvalue_matrix: dict[str, dict[str, float]],
    ) -> list[CorrelationPair]:
        """æå–æ‰€æœ‰ä¸é‡å¤çš„å˜é‡é…å¯¹ã€‚"""
        pairs: list[CorrelationPair] = []
        for i, col1 in enumerate(columns):
            for col2 in columns[i + 1 :]:
                coeff = corr_matrix.get(col1, {}).get(col2, 0.0)
                p_val = pvalue_matrix.get(col1, {}).get(col2, 1.0)
                if not math.isfinite(coeff):
                    coeff = 0.0
                if not math.isfinite(p_val):
                    p_val = 1.0
                pairs.append(
                    CorrelationPair(
                        var1=col1,
                        var2=col2,
                        coefficient=coeff,
                        p_value=p_val,
                        strength=self._classify_strength(coeff),
                    )
                )
        pairs.sort(key=lambda p: abs(p.coefficient), reverse=True)
        return pairs

    @staticmethod
    def _classify_strength(coefficient: float) -> str:
        """æ ¹æ®ç»å¯¹å€¼åˆ†ç±»ç›¸å…³å¼ºåº¦ã€‚"""
        abs_r = abs(coefficient)
        if abs_r >= 0.7:
            return "strong"
        elif abs_r >= 0.4:
            return "moderate"
        elif abs_r >= 0.2:
            return "weak"
        return "negligible"

    @staticmethod
    def _apply_correction(
        pairs: list[CorrelationPair],
        method: str,
        n_comparisons: int,
        alpha: float,
    ) -> None:
        """å¯¹é…å¯¹ p å€¼åšå¤šé‡æ¯”è¾ƒæ ¡æ­£ã€‚"""
        if method == "bonferroni" and n_comparisons > 1:
            adjusted_alpha = alpha / n_comparisons
            for pair in pairs:
                pair.p_adjusted = min(pair.p_value * n_comparisons, 1.0)
                pair.significant = pair.p_value < adjusted_alpha
        else:
            for pair in pairs:
                pair.p_adjusted = pair.p_value
                pair.significant = pair.p_value < alpha

    async def _create_heatmap(
        self,
        session: Session,
        dataset_name: str,
        columns: list[str],
        method: str,
        corr_matrix: dict[str, dict[str, float]] | None = None,
    ) -> Any:
        """åˆ›å»ºç›¸å…³çŸ©é˜µçƒ­åŠ›å›¾ã€‚

        å¦‚æœæä¾›äº† corr_matrixï¼Œå…ˆå°†å…¶å†™å…¥ session çš„ä¸´æ—¶æ•°æ®ä¸­ï¼Œ
        è®© create_chart ä½¿ç”¨ä¸æ–‡æœ¬ä¸€è‡´çš„æ•°æ®ï¼ˆç»è¿‡ dropna çš„ç»“æœï¼‰ã€‚
        """
        registry = self._get_registry()
        # å¦‚æœæœ‰é¢„è®¡ç®—çŸ©é˜µï¼Œæ„å»ºä¸€ä¸ª DataFrame å†™å…¥ä¼šè¯ä¾›å›¾è¡¨ä½¿ç”¨
        if corr_matrix:
            corr_df = pd.DataFrame(corr_matrix)
            corr_df = corr_df.reindex(index=columns, columns=columns)
            temp_name = f"_corr_matrix_{dataset_name}"
            session.datasets[temp_name] = corr_df
            chart_dataset = temp_name
        else:
            chart_dataset = dataset_name
        try:
            result = await registry.execute(
                "create_chart",
                session,
                dataset_name=chart_dataset,
                chart_type="heatmap",
                columns=columns,
                title=f"{method.title()} ç›¸å…³çŸ©é˜µ",
            )
            return result
        except Exception:
            return None
        finally:
            # æ¸…ç†ä¸´æ—¶æ•°æ®é›†
            if corr_matrix:
                session.datasets.pop(f"_corr_matrix_{dataset_name}", None)

    @staticmethod
    def _record_enriched_result(
        session: Session,
        dataset_name: str,
        result: CorrelationAnalysisResult,
    ) -> None:
        """å°†ç›¸å…³æ€§åˆ†æå¯Œä¿¡æ¯è®°å½•åˆ° AnalysisMemoryã€‚"""
        from nini.tools.statistics.base import _record_stat_result

        # ä¸ºæ¯å¯¹æ˜¾è‘—ç›¸å…³è®°å½•ä¸€æ¡
        for pair in result.significant_pairs:
            _record_stat_result(
                session,
                dataset_name,
                test_name=f"{result.method.title()} ç›¸å…³æ€§ ({pair.var1} â†” {pair.var2})",
                message=(
                    f"r = {pair.coefficient:.3f}, p_adj = {pair.p_adjusted:.4f}, "
                    f"å¼ºåº¦: {pair.strength}"
                ),
                test_statistic=pair.coefficient,
                p_value=pair.p_value,
                effect_size=abs(pair.coefficient),
                effect_type="r",
                significant=pair.significant,
            )

        # å¦‚æœæ²¡æœ‰æ˜¾è‘—å¯¹ï¼Œè®°å½•ä¸€æ¡æ±‡æ€»
        if not result.significant_pairs:
            _record_stat_result(
                session,
                dataset_name,
                test_name=f"{result.method.title()} ç›¸å…³æ€§åˆ†æ",
                message=f"åˆ†æ {result.n_variables} ä¸ªå˜é‡ï¼Œæœªå‘ç°æ˜¾è‘—ç›¸å…³",
            )

    def _generate_interpretation(self, result: CorrelationAnalysisResult) -> str:
        """ç”Ÿæˆè§£é‡Šæ€§æŠ¥å‘Šã€‚"""
        parts: list[str] = []

        parts.append("## åˆ†ææ–¹æ³•")
        parts.append(f"é€‰æ‹©æ–¹æ³•: {result.method.title()}")
        parts.append(f"é€‰æ‹©ç†ç”±: {result.method_reason}")
        parts.append(f"å˜é‡æ•°: {result.n_variables}, æœ‰æ•ˆæ ·æœ¬é‡: {result.sample_size}")

        if result.correction_method and result.correction_method != "none":
            parts.append(
                f"å¤šé‡æ¯”è¾ƒæ ¡æ­£: {result.correction_method.title()}"
                f"ï¼ˆ{len(result.all_pairs)} æ¬¡æ¯”è¾ƒï¼‰"
            )

        parts.append("\n## æ˜¾è‘—ç›¸å…³")
        if result.significant_pairs:
            for pair in result.significant_pairs:
                direction = "æ­£ç›¸å…³" if pair.coefficient > 0 else "è´Ÿç›¸å…³"
                strength_cn = {
                    "strong": "å¼º",
                    "moderate": "ä¸­ç­‰",
                    "weak": "å¼±",
                    "negligible": "æå¼±",
                }.get(pair.strength, "")
                p_display = pair.p_adjusted if pair.p_adjusted is not None else pair.p_value
                parts.append(
                    f"- {pair.var1} â†” {pair.var2}: "
                    f"r = {pair.coefficient:.3f} ({strength_cn}{direction}), "
                    f"p_adj = {p_display:.4f}"
                )
        else:
            parts.append("æœªå‘ç°æ˜¾è‘—ç›¸å…³ï¼ˆæ ¡æ­£å p < {:.2f}ï¼‰ã€‚".format(result.alpha))

        parts.append("\n## ç»“è®º")
        n_sig = len(result.significant_pairs)
        n_total = len(result.all_pairs)
        if n_sig > 0:
            strong_pairs = [p for p in result.significant_pairs if p.strength == "strong"]
            if strong_pairs:
                names = [f"{p.var1}-{p.var2}" for p in strong_pairs[:3]]
                parts.append(f"å‘ç° {n_sig}/{n_total} å¯¹æ˜¾è‘—ç›¸å…³ï¼Œå…¶ä¸­å¼ºç›¸å…³: {', '.join(names)}ã€‚")
            else:
                parts.append(f"å‘ç° {n_sig}/{n_total} å¯¹æ˜¾è‘—ç›¸å…³ï¼Œä½†å‡ä¸ºä¸­ç­‰æˆ–å¼±ç›¸å…³ã€‚")
        else:
            parts.append(f"åœ¨ {n_total} å¯¹å˜é‡ä¸­æœªå‘ç°æ˜¾è‘—ç›¸å…³å…³ç³»ã€‚")

        return "\n".join(parts)
